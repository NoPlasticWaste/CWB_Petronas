{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TextDocumentBatchStatistics' from 'azure.cognitiveservices.language.textanalytics.models' (/Users/vivien/Documents/Petronas/CWB_Petronas/env/lib/python3.11/site-packages/azure/cognitiveservices/language/textanalytics/models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcognitiveservices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlanguage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtextanalytics\u001b[39;00m \u001b[39mimport\u001b[39;00m TextAnalyticsClient\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcognitiveservices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlanguage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtextanalytics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     TextDocumentBatchStatistics,\n\u001b[1;32m      4\u001b[0m     TextDocumentInput,\n\u001b[1;32m      5\u001b[0m     TextAnalyticsApiKeyCredential,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtextanalytics\u001b[39;00m \u001b[39mimport\u001b[39;00m TextDocumentBatchStatistics\n\u001b[1;32m      9\u001b[0m \u001b[39m# Set your Text Analytics API endpoint and key\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TextDocumentBatchStatistics' from 'azure.cognitiveservices.language.textanalytics.models' (/Users/vivien/Documents/Petronas/CWB_Petronas/env/lib/python3.11/site-packages/azure/cognitiveservices/language/textanalytics/models/__init__.py)"
     ]
    }
   ],
   "source": [
    "from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient\n",
    "from azure.cognitiveservices.language.textanalytics.models import (\n",
    "    TextDocumentBatchStatistics,\n",
    "    TextDocumentInput,\n",
    "    TextAnalyticsApiKeyCredential,\n",
    ")\n",
    "from azure.ai.textanalytics import TextDocumentBatchStatistics\n",
    "\n",
    "# Set your Text Analytics API endpoint and key\n",
    "key = \"0cebfaee5b5a4c3a9c4d504320150da0\"\n",
    "endpoint = \"https://cwb-petronas-vivien.cognitiveservices.azure.com/\"\n",
    "\n",
    "# Create a Text Analytics client\n",
    "credentials = TextAnalyticsApiKeyCredential(key)\n",
    "text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=credentials)\n",
    "\n",
    "# Set the container name and Blob storage connection string\n",
    "container_name = 'reports'\n",
    "connection_string = 'https://petronashackathon.blob.core.windows.net/reports'\n",
    "\n",
    "# Get the Blob service client\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# List the PDF files in the container and analyze the key phrases in each file\n",
    "for blob in blob_service_client.list_blobs(container_name):\n",
    "    if blob.name.endswith('.pdf'):\n",
    "        # Get the PDF file as a stream\n",
    "        blob_client = blob_service_client.get_blob_client(container_name, blob.name)\n",
    "        stream = blob_client.download_blob().content_as_bytes()\n",
    "\n",
    "        # Extract the text from the PDF file using PyPDF2 or pdfminer\n",
    "        # ...\n",
    "\n",
    "        # Analyze the key phrases in the text using the Text Analytics API\n",
    "        documents = [TextDocumentInput(id=blob.name, text=text)]\n",
    "        response = text_analytics_client.extract_key_phrases(documents=documents)\n",
    "\n",
    "        # Print the key phrases for each document\n",
    "        for document in response.documents:\n",
    "            print('Document: {}'.format(document.id))\n",
    "            print('Key phrases:')\n",
    "            for phrase in document.key_phrases:\n",
    "                print('- {}'.format(phrase))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
